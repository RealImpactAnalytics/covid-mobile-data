{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production of indicators for the COVID19 Mobility Task Force\n",
    "\n",
    "In this notebook we produce indicators for the [COVID19 Mobility Task Force](https://github.com/worldbank/covid-mobile-data).\n",
    "\n",
    "[Flowminder](https://covid19.flowminder.org) indicators are produced to increase the availability of comparable datasets across countries, and have been copied without modification from the [Flowminder COVID-19 github repository](https://github.com/Flowminder/COVID-19) (except for the start and end dates). These have been supplemented by a set of *priority* indicators with data for ingestion into the dashboard in this repository.\n",
    "\n",
    "In this notebook we produce indicators in the following four steps:\n",
    "\n",
    "- **Import code**: The code for the aggregation is included in the 'custom_aggregation' and 'flowminder_aggregation' scripts\n",
    "- **Import data**: \n",
    "To set up the data import we need to place the CDR data files into the `data/new/CC/telco/` folder, where we replace `CC` with the country code and `telco` with the company abbreviation. \n",
    "We also need to place csv files with the tower-region mapping and distance matrices into the `data/support-data/CC/telco/geofiles` folder, and then modify the `data/support_data/config_file.py` to specify:\n",
    "    - *geofiles*: the names of the geofiles, \n",
    "    - *country_code*: country code and company abbreviation,\n",
    "    - *telecom_alias*: the path to the `data` folder,\n",
    "    - *data_paths*: the names to the subfolders in `data/new/CC/telco/` that hold the csv files. Simply change this to `[*]` if you didn't create subfolders and want to load all files.\n",
    "    - *dates*: set the start and end date of the data you want to produce the indicators for.\n",
    "    \n",
    "Find more information about the `config_file.py` settings see the [github page](https://github.com/worldbank/covid-mobile-data/tree/master/cdr-aggregation).\n",
    "    \n",
    "- **Run aggregations**: By default, we produce all flowminder and priority indicators. We've included 4 re-tries in case of failure, which we have experienced to help on databricks but is probably irrelevant in other settings. Note that before you can re-run these aggregations, you need to move the csv outputs that have been saved in `data/results/CC/telco/` in previous runs to another folder, else these indicators will be skipped. This prevents you from accidentally overwriting previous results. This way you can also delete the files only for the indicators you want to re-produce, and skip any indicatos you don't want to re-produce.\n",
    "\n",
    "The outcome of this effort will be used to inform policy making using a [mobility indicator dashboard](https://github.com/worldbank/covid-mobile-data/tree/master/dashboard-dataviz)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section switches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocess raw csv data\n",
    "STANDARDIZE_CSV_FILES = True\n",
    "SAVE_STAND_PARQUE_FILES = True    \n",
    "\n",
    "# Load preprocesed data\n",
    "LOAD_FULL_CSV_DATA = True\n",
    "# Alternatively, specify and load hive table\n",
    "SPECIFY_HIVE_TABLE = False\n",
    "\n",
    "# Instead of loading the full data, use these to create and load a\n",
    "# random sample of users\n",
    "CREATE_SAMPLE = False\n",
    "LOAD_SAMPLE = False\n",
    "\n",
    "# Create standard GIS files for aggregations. This step does not \n",
    "# depend on the CDR data directly, but is needed to run the aggregations.\n",
    "# This only has to be done once per country and operator\n",
    "\n",
    "TOWER_CLUSTER = True\n",
    "VORONOY_TESSELATION = True\n",
    "\n",
    "# Run aggregation for different admin levels\n",
    "RUN_AGGREGATION_ADMIN2 = False\n",
    "RUN_AGGREGATION_ADMIN3 = False\n",
    "RUN_AGGREGATION_TOWER_CLUSTER = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.DataSource import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = '../config_file.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open(config_file).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark mode is local\n",
      "\n",
      "Basepath: /home/jovyan/work/data\n",
      "Country and company path: country/operator\n",
      "Paths for datafiles: ['*.csv']\n",
      "Geofiles: {'tower_sites': 'LR_sites.csv', 'admin1': 'LR_admin1_shapefile.csv', 'admin1_tower_map': 'LR_admin1_tower_mapping.csv', 'admin2': 'LR_admin2_shapefile.csv', 'admin2_tower_map': 'LR_admin2_tower_mapping.csv', 'admin3': 'LR_admin3_shapefile.csv', 'admin3_tower_map': 'LR_admin3_tower_mapping.csv', 'distances': 'country_distances_pd_long.csv'}\n",
      "Load options: {'seperator': ',', 'header': 'false', 'mode': 'DROPMALFORMED', 'datemask': 'yyyy-MM-dd HH:mm:ss'}\n",
      "Load schema: StructType(List(StructField(msisdn,StringType,true),StructField(call_datetime,StringType,true),StructField(location_id,StringType,true)))\n",
      "Filenames: {'parquetfile': 'data-file.parquet'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds = DataSource(datasource_configs)\n",
    "ds.show_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.setup import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CDR data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process/standardize raw data, save as parquet, and then load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-----------+----------+\n",
      "|    msisdn|      call_datetime|location_id| call_date|\n",
      "+----------+-------------------+-----------+----------+\n",
      "|2311234567|2020-03-01 10:35:00|       1102|2020-03-01|\n",
      "|2311234567|2020-03-01 10:35:00|       3102|2020-03-01|\n",
      "|2311234567|2020-03-01 10:35:00|       1150|2020-03-01|\n",
      "|2311234567|2020-03-01 10:35:00|       3150|2020-03-01|\n",
      "|2311234567|2020-03-01 10:35:00|       2150|2020-03-01|\n",
      "|2311234567|2020-03-01 10:35:00|       4017|2020-03-01|\n",
      "|2311234567|2020-03-01 10:35:00|       3017|2020-03-01|\n",
      "|2311234567|2020-03-01 10:35:00|       1017|2020-03-01|\n",
      "|2311234567|2020-03-01 10:35:00|       5017|2020-03-01|\n",
      "|2311234567|2020-03-01 10:35:00|       4162|2020-03-01|\n",
      "+----------+-------------------+-----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# These processes only have to be done once for CSV batch. \n",
    "# Once .parquet files are saved for that CSV batch, aggregations \n",
    "# can run on them. \n",
    "\n",
    "# Load and standardize raw CDR csvs\n",
    "if STANDARDIZE_CSV_FILES:\n",
    "    ds.standardize_csv_files(show=True)\n",
    "\n",
    "# Export as parquet files \n",
    "if SAVE_STAND_PARQUE_FILES:\n",
    "    ds.save_as_parquet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load standardized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full set of parquet files\n",
    "if LOAD_FULL_CSV_DATA:\n",
    "    ds.load_standardized_parquet_file()\n",
    "# Specify and load hive data\n",
    "elif SPECIFY_HIVE_TABLE:\n",
    "    ds.parquet_df = ds.spark.sql(\"\"\"SELECT {} AS msisdn, \n",
    "                                           {} AS call_datetime, \n",
    "                                           {} AS location_id FROM {}\"\"\".format(ds.hive_vars['msisdn'],\n",
    "                                                                               ds.hive_vars['call_datetime'],\n",
    "                                                                               ds.hive_vars['location_id'],))\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or load a sample file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this in case you want to sample the data and run the code on the sample\n",
    "if CREATE_SAMPLE:\n",
    "    ds.sample_and_save(number_of_ids=1000)\n",
    "\n",
    "# This will replace ds.parque_df with the created sample\n",
    "if LOAD_SAMPLE:\n",
    "    ds.load_sample('sample_feb_mar2020')\n",
    "    ds.parquet_df = ds.sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load geo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.load_geo_csvs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.create_gpds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/session.py:714: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n",
      "  An error occurred while calling z:org.apache.spark.sql.api.python.PythonSQLUtils.readArrowStreamFromFile.\n",
      ": java.lang.IllegalArgumentException\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:334)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:543)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$3.readNextBatch(ArrowConverters.scala:243)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$3.<init>(ArrowConverters.scala:229)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.getBatchesFromStream(ArrowConverters.scala:228)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$readArrowStreamFromFile$2.apply(ArrowConverters.scala:216)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$readArrowStreamFromFile$2.apply(ArrowConverters.scala:214)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2543)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.readArrowStreamFromFile(ArrowConverters.scala:214)\n",
      "\tat org.apache.spark.sql.api.python.PythonSQLUtils$.readArrowStreamFromFile(PythonSQLUtils.scala:46)\n",
      "\tat org.apache.spark.sql.api.python.PythonSQLUtils.readArrowStreamFromFile(PythonSQLUtils.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "## Use this in case you want to cluster the towers and create a distance matrix\n",
    "if TOWER_CLUSTER:\n",
    "    from modules.tower_clustering import *\n",
    "    clusterer = tower_clusterer(ds, 'admin2', 'ADM2_PCODE') # ID_2, region\n",
    "    ds.admin2_tower_map, ds.distances = clusterer.cluster_towers()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cell_id=1102, region='5db86405f9857515c8f32b43'),\n",
       " Row(cell_id=3102, region='5db86405f9857515c8f32b43')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.admin3_tower_map.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADM0_EN</th>\n",
       "      <th>ADM0_PCODE</th>\n",
       "      <th>ADM1_EN</th>\n",
       "      <th>ADM1_PCODE</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Liberia</td>\n",
       "      <td>LR</td>\n",
       "      <td>Central Region</td>\n",
       "      <td>5db86405f9857515c8f32acd</td>\n",
       "      <td>POLYGON ((-9.94021 6.48858, -9.94021 6.48858, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Liberia</td>\n",
       "      <td>LR</td>\n",
       "      <td>Southern Region</td>\n",
       "      <td>5db86405f9857515c8f32ad1</td>\n",
       "      <td>POLYGON ((-7.40067 5.51397, -7.40016 5.51383, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Liberia</td>\n",
       "      <td>LR</td>\n",
       "      <td>Montserrado South</td>\n",
       "      <td>5db86405f9857515c8f32ace</td>\n",
       "      <td>POLYGON ((-10.76417 6.39671, -10.76411 6.39674...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Liberia</td>\n",
       "      <td>LR</td>\n",
       "      <td>Montserrado North</td>\n",
       "      <td>5db86405f9857515c8f32acf</td>\n",
       "      <td>POLYGON ((-11.43728 6.87560, -11.44230 6.87865...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Liberia</td>\n",
       "      <td>LR</td>\n",
       "      <td>Northern Region</td>\n",
       "      <td>5db86405f9857515c8f32ad0</td>\n",
       "      <td>POLYGON ((-8.71029 7.60592, -8.71026 7.60603, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ADM0_EN ADM0_PCODE            ADM1_EN                ADM1_PCODE  \\\n",
       "0  Liberia         LR     Central Region  5db86405f9857515c8f32acd   \n",
       "1  Liberia         LR    Southern Region  5db86405f9857515c8f32ad1   \n",
       "2  Liberia         LR  Montserrado South  5db86405f9857515c8f32ace   \n",
       "3  Liberia         LR  Montserrado North  5db86405f9857515c8f32acf   \n",
       "4  Liberia         LR    Northern Region  5db86405f9857515c8f32ad0   \n",
       "\n",
       "                                            geometry  \n",
       "0  POLYGON ((-9.94021 6.48858, -9.94021 6.48858, ...  \n",
       "1  POLYGON ((-7.40067 5.51397, -7.40016 5.51383, ...  \n",
       "2  POLYGON ((-10.76417 6.39671, -10.76411 6.39674...  \n",
       "3  POLYGON ((-11.43728 6.87560, -11.44230 6.87865...  \n",
       "4  POLYGON ((-8.71029 7.60592, -8.71026 7.60603, ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusterer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/session.py:714: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n",
      "  An error occurred while calling z:org.apache.spark.sql.api.python.PythonSQLUtils.readArrowStreamFromFile.\n",
      ": java.lang.IllegalArgumentException\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:334)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:543)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$3.readNextBatch(ArrowConverters.scala:243)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$3.<init>(ArrowConverters.scala:229)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.getBatchesFromStream(ArrowConverters.scala:228)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$readArrowStreamFromFile$2.apply(ArrowConverters.scala:216)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$readArrowStreamFromFile$2.apply(ArrowConverters.scala:214)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2543)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.readArrowStreamFromFile(ArrowConverters.scala:214)\n",
      "\tat org.apache.spark.sql.api.python.PythonSQLUtils$.readArrowStreamFromFile(PythonSQLUtils.scala:46)\n",
      "\tat org.apache.spark.sql.api.python.PythonSQLUtils.readArrowStreamFromFile(PythonSQLUtils.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from modules.tower_clustering import *\n",
    "clusterer = tower_clusterer(ds, 'admin3', 'ADM3_PCODE') # ADM3_PCODE\n",
    "ds.admin3_tower_map, ds.distances  = clusterer.cluster_towers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/session.py:714: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n",
      "  An error occurred while calling z:org.apache.spark.sql.api.python.PythonSQLUtils.readArrowStreamFromFile.\n",
      ": java.lang.IllegalArgumentException\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:334)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:543)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$3.readNextBatch(ArrowConverters.scala:243)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$3.<init>(ArrowConverters.scala:229)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.getBatchesFromStream(ArrowConverters.scala:228)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$readArrowStreamFromFile$2.apply(ArrowConverters.scala:216)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$readArrowStreamFromFile$2.apply(ArrowConverters.scala:214)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2543)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.readArrowStreamFromFile(ArrowConverters.scala:214)\n",
      "\tat org.apache.spark.sql.api.python.PythonSQLUtils$.readArrowStreamFromFile(PythonSQLUtils.scala:46)\n",
      "\tat org.apache.spark.sql.api.python.PythonSQLUtils.readArrowStreamFromFile(PythonSQLUtils.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n",
      "/usr/local/spark/python/pyspark/sql/session.py:714: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n",
      "  An error occurred while calling z:org.apache.spark.sql.api.python.PythonSQLUtils.readArrowStreamFromFile.\n",
      ": java.lang.IllegalArgumentException\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:334)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:543)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$3.readNextBatch(ArrowConverters.scala:243)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$3.<init>(ArrowConverters.scala:229)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.getBatchesFromStream(ArrowConverters.scala:228)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$readArrowStreamFromFile$2.apply(ArrowConverters.scala:216)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$readArrowStreamFromFile$2.apply(ArrowConverters.scala:214)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2543)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.readArrowStreamFromFile(ArrowConverters.scala:214)\n",
      "\tat org.apache.spark.sql.api.python.PythonSQLUtils$.readArrowStreamFromFile(PythonSQLUtils.scala:46)\n",
      "\tat org.apache.spark.sql.api.python.PythonSQLUtils.readArrowStreamFromFile(PythonSQLUtils.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "## Use this in case you want to create a voronoi tesselation\n",
    "if VORONOY_TESSELATION:\n",
    "    from modules.voronoi import *\n",
    "    voronoi = voronoi_maker(ds, 'admin3', 'ADM3_PCODE')\n",
    "    ds.voronoi = voronoi.make_voronoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Priority indicators for admin2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_AGGREGATION_ADMIN2:\n",
    "    agg_priority_admin2 = priority_aggregator(result_stub = '/admin2/priority',\n",
    "                                   datasource = ds,\n",
    "                                   regions = 'admin2_tower_map')\n",
    "\n",
    "    agg_priority_admin2.attempt_aggregation()\n",
    "    # You can produce a subset of indicators by passing a dictionary with keys as a file name and a list with indicator ids and aggregation levels.\n",
    "    #agg_priority_admin2.attempt_aggregation(indicators_to_produce = {'unique_subscribers_per_day' : ['unique_subscribers', 'day'],\n",
    "    #                                                                  'percent_of_all_subscribers_active_per_day' : ['percent_of_all_subscribers_active', 'day'],\n",
    "    #                                                                  'origin_destination_connection_matrix_per_day' : ['origin_destination_connection_matrix', 'day'],\n",
    "    #                                                                  'mean_distance_per_day' : ['mean_distance', 'day'],\n",
    "    #                                                                  'mean_distance_per_week' : ['mean_distance', 'week'],\n",
    "    #                                                                  'origin_destination_matrix_time_per_day' : ['origin_destination_matrix_time', 'day'],\n",
    "    #                                                                  'home_vs_day_location_per_day' : ['home_vs_day_location_per_day', ['day','week']],\n",
    "    #                                                                  'home_vs_day_location_per_day' : ['home_vs_day_location_per_day', ['day','month']]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Priority indicators for admin3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_AGGREGATION_ADMIN3:\n",
    "    agg_priority_admin3 = priority_aggregator(result_stub = '/admin3/priority',\n",
    "                                datasource = ds,\n",
    "                                regions = 'admin3_tower_map')\n",
    "\n",
    "    agg_priority_admin3.attempt_aggregation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Priority indicators for tower-cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_AGGREGATION_TOWER_CLUSTER:\n",
    "    agg_priority_tower = priority_aggregator(result_stub = '/voronoi/priority',\n",
    "                                   datasource = ds,\n",
    "                                   regions = 'voronoi_tower_map')\n",
    "\n",
    "    agg_priority_tower.attempt_aggregation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_AGGREGATION_TOWER_CLUSTER:\n",
    "    agg_priority_tower_harare = priority_aggregator(result_stub = '/voronoi/priority/harare',\n",
    "                                   datasource = ds,\n",
    "                                   regions = 'voronoi_tower_map_harare')\n",
    "\n",
    "    agg_priority_tower_harare.attempt_aggregation(indicators_to_produce = {'origin_destination_connection_matrix_per_day' : ['origin_destination_connection_matrix', 'day']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_AGGREGATION_TOWER_CLUSTER:\n",
    "    agg_priority_tower_bulawayo = priority_aggregator(result_stub = '/voronoi/priority/bulawayo',\n",
    "                                   datasource = ds,\n",
    "                                   regions = 'voronoi_tower_map_bulawayo')\n",
    "\n",
    "    agg_priority_tower_bulawayo.attempt_aggregation(indicators_to_produce = {'origin_destination_connection_matrix_per_day' : ['origin_destination_connection_matrix', 'day']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook aggregation_master.ipynb to script\n",
      "[NbConvertApp] Writing 9644 bytes to aggregation_master.py\n",
      "[NbConvertApp] Converting notebook folder_setup.ipynb to script\n",
      "[NbConvertApp] Writing 525 bytes to folder_setup.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script *.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame([[np.nan,1, 2],[0,1,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ds.spark.createDataFrame([[None,1, 1,2],[2,2,2,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.groupby('_4').sum().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.withColumn('f', F.col('_1') + F.col('_2')).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
